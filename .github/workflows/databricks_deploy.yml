name: Deploy PySpark Project to Databricks

on:
  push:
    branches:
      - main

jobs:
  build-and-deploy:
    runs-on: ubuntu-latest

    steps:
      # 1️⃣ Checkout repo
      - name: Checkout code
        uses: actions/checkout@v3

      # 2️⃣ Set up Python
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'

      # 3️⃣ Install dependencies and build wheel
      - name: Install dependencies and build wheel
        run: |
          python -m pip install --upgrade pip setuptools wheel
          pip install -r requirements.txt
          python setup.py bdist_wheel
          ls -lh dist/

      # 4️⃣ Install NEW Databricks CLI
      - name: Install Databricks CLI (new)
        run: |
          curl -fsSL https://raw.githubusercontent.com/databricks/cli/main/install.sh | bash
          echo "CLI installed successfully"
          databricks version

      # 5️⃣ Upload wheel to DBFS
      - name: Upload wheel to DBFS
        env:
          DATABRICKS_HOST: ${{ secrets.DATABRICKS_HOST }}
          DATABRICKS_TOKEN: ${{ secrets.DATABRICKS_TOKEN }}
        run: |
          WHEEL_PATH=$(ls dist/*.whl | head -n 1)
          echo "Uploading $WHEEL_PATH to DBFS..."
          databricks fs cp "$WHEEL_PATH" dbfs:/FileStore/wheels/ --overwrite

      # 6️⃣ Deploy Databricks Job
      - name: Deploy Databricks Job
        env:
          DATABRICKS_HOST: ${{ secrets.DATABRICKS_HOST }}
          DATABRICKS_TOKEN: ${{ secrets.DATABRICKS_TOKEN }}
        run: |
          echo "Creating or updating Databricks job..."
          databricks jobs create --json @databricks_job.json || echo "Job already exists"

      # 7️⃣ (Optional) Run job immediately
      - name: Run Databricks Job
        env:
          DATABRICKS_HOST: ${{ secrets.DATABRICKS_HOST }}
          DATABRICKS_TOKEN: ${{ secrets.DATABRICKS_TOKEN }}
        run: |
          JOB_ID=$(databricks jobs list --output json | jq -r '.jobs[] | select(.name=="PySpark_ETL_Job") | .id')
          echo "Running job $JOB_ID..."
          databricks jobs run-now --job-id "$JOB_ID"
