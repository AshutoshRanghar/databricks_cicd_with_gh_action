name: Deploy PySpark Project to Databricks

on:
  push:
    branches:
      - main

jobs:
  build-and-deploy:
    runs-on: ubuntu-latest

    steps:
      # 1️⃣ Checkout repository
      - name: Checkout code
        uses: actions/checkout@v3

      # 2️⃣ Set up Python
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'

      # 3️⃣ Install dependencies
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip setuptools wheel
          pip install -r requirements.txt

      # 4️⃣ Build the .whl package
      - name: Build wheel package
        run: |
          python setup.py bdist_wheel
          echo "Built wheel:"
          ls -lh dist/

      # 5️⃣ Install Databricks CLI
      - name: Install Databricks CLI
        run: |
          pip install databricks-cli==0.18.0

      # 6️⃣ Configure Databricks CLI auth
      - name: Configure Databricks CLI
        run: |
          mkdir -p ~/.databricks
          echo "[DEFAULT]" > ~/.databricks/config
          echo "host = $DATABRICKS_HOST" >> ~/.databricks/config
          echo "token = $DATABRICKS_TOKEN" >> ~/.databricks/config
        env:
          DATABRICKS_HOST: ${{ secrets.DATABRICKS_HOST }}
          DATABRICKS_TOKEN: ${{ secrets.DATABRICKS_TOKEN }}

      # 7️⃣ Upload the wheel to DBFS
      - name: Upload wheel to DBFS
        env:
          DATABRICKS_HOST: ${{ secrets.DATABRICKS_HOST }}
          DATABRICKS_TOKEN: ${{ secrets.DATABRICKS_TOKEN }}
        run: |
          WHEEL_PATH=$(ls dist/*.whl | head -n 1)
          echo "Uploading $WHEEL_PATH to DBFS..."
          databricks fs cp "$WHEEL_PATH" dbfs:/FileStore/wheels/ --overwrite

      # 8️⃣ Deploy (create or overwrite) Databricks job
      - name: Deploy Databricks job
        run: |
          databricks jobs create --json-file databricks_job.json || \
          databricks jobs reset --job-id $(databricks jobs list --output JSON | jq -r '.jobs[] | select(.settings.name=="PySpark_ETL_Job") | .job_id') --json-file databricks_job.json
